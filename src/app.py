from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
from typing import Optional, List, Dict
import redis
import json
import os
from datetime import datetime
from dotenv import load_dotenv
import openai

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# åˆå§‹åŒ–FastAPIåº”ç”¨
app = FastAPI(
    title="å¾®ä¿¡æœ‹å‹åœˆè‡ªåŠ¨å›å¤æœåŠ¡",
    description="æä¾›æœ‹å‹åœˆè‡ªåŠ¨å›å¤ç”ŸæˆåŠŸèƒ½",
    version="1.0.0"
)

# åˆå§‹åŒ–Redisè¿æ¥ï¼ˆå¦‚æœç¯å¢ƒä¸­æœ‰é…ç½®ï¼‰
REDIS_URL = os.getenv("REDIS_URL", "redis://localhost:6379/0")
REDIS_AVAILABLE = False
redis_client = None

try:
    redis_client = redis.from_url(REDIS_URL, decode_responses=True, socket_connect_timeout=5)
    redis_client.ping()
    REDIS_AVAILABLE = True
    print("Redisè¿æ¥æˆåŠŸ")
except Exception as e:
    print(f"Redisè¿æ¥å¤±è´¥ï¼Œå°†ä½¿ç”¨å†…å­˜å­˜å‚¨: {str(e)}")

# å†…å­˜å­˜å‚¨ï¼Œç”¨äºç¼“å­˜ç”¨æˆ·çš„å›å¤å†å²
memory_storage: Dict[str, List[Dict]] = {}

# é…ç½®OpenAI APIï¼ˆæ ¹æ®å®é™…ä½¿ç”¨çš„å¤§æ¨¡å‹è°ƒæ•´ï¼‰
openai.api_key = os.getenv("OPENAI_API_KEY", "")

# å®šä¹‰æ•°æ®æ¨¡å‹
class ReplyRequest(BaseModel):
    circle_content: str = Field(..., description="æœ‹å‹åœˆå†…å®¹")
    reply_style: str = Field(..., description="å›å¤é£æ ¼ï¼šå¹½é»˜ï¼Œä¸¥è‚ƒï¼Œæš§æ˜§ï¼Œæ¸©é¦¨ï¼Œæ‰¹è¯„")
    user_id: str = Field(..., description="ç”¨æˆ·å”¯ä¸€æ ‡è¯†")
    post_id: str = Field(..., description="æœ‹å‹åœˆå¸–å­å”¯ä¸€æ ‡è¯†")
    previous_replies: Optional[List[str]] = Field(default=[], description="ä¹‹å‰çš„å›å¤å†…å®¹åˆ—è¡¨")

class EmotionAnalysisResult(BaseModel):
    emotion_type: str = Field(..., description="æƒ…ç»ªç±»å‹")
    negative_score: int = Field(..., ge=1, le=10, description="è´Ÿé¢ç¨‹åº¦è¯„åˆ†ï¼Œ1-10åˆ†")
    emotion_description: str = Field(..., description="æƒ…ç»ªæè¿°")

class ReplyResponse(BaseModel):
    reply_content: str = Field(..., description="ç”Ÿæˆçš„å›å¤å†…å®¹")
    is_first_reply: bool = Field(..., description="æ˜¯å¦æ˜¯ç¬¬ä¸€æ¬¡å›å¤")
    timestamp: datetime = Field(..., description="ç”Ÿæˆæ—¶é—´æˆ³")
    emotion_analysis: EmotionAnalysisResult = Field(..., description="æƒ…æ„Ÿåˆ†æç»“æœ")

class AdDetectionRequest(BaseModel):
    circle_content: str = Field(..., description="æœ‹å‹åœˆå†…å®¹")
    user_id: str = Field(..., description="ç”¨æˆ·å”¯ä¸€æ ‡è¯†")
    post_id: str = Field(..., description="æœ‹å‹åœˆå¸–å­å”¯ä¸€æ ‡è¯†")

class AdDetectionResponse(BaseModel):
    is_ad: bool = Field(..., description="æ˜¯å¦ä¸ºå¹¿å‘Š")
    response_text: str = Field(..., description="å“åº”æ–‡æœ¬")
    confidence: float = Field(..., ge=0.0, le=1.0, description="æ£€æµ‹ç½®ä¿¡åº¦")
    timestamp: datetime = Field(..., description="æ£€æµ‹æ—¶é—´æˆ³")

# éªŒè¯å›å¤é£æ ¼
valid_styles = ["å¹½é»˜", "ä¸¥è‚ƒ", "æš§æ˜§", "æ¸©é¦¨", "æ‰¹è¯„"]

# å¹¿å‘Šæ£€æµ‹å…³é”®è¯åˆ—è¡¨
ad_keywords = [
    "ä¼˜æƒ ", "æŠ˜æ‰£", "ä¿ƒé”€", "ç‰¹ä»·", "é™æ—¶", "ç§’æ€", "æŠ¢è´­",
    "å…è´¹é¢†å–", "è½¬å‘æŠ½å¥–", "æ·»åŠ å¾®ä¿¡", "æ‰«ç å…³æ³¨", "åŠ ç¾¤",
    "æŠ•èµ„", "ç†è´¢", "èµšé’±", "å…¼èŒ", "å‰¯ä¸š", "æ—¥å…¥", "æœˆå…¥",
    "ä»£ç†", "åŠ ç›Ÿ", "æ‹›å•†", "åˆä¼™äºº", "ä¼šå‘˜", "VIP", "å¥—é¤",
    "å’¨è¯¢ç”µè¯", "è”ç³»æ–¹å¼", "å¾®ä¿¡", "QQ", "ç”µè¯", "æ‰‹æœºå·",
    "ç½‘å€", "é“¾æ¥", "ç½‘å€æ˜¯", "é“¾æ¥æ˜¯", "ç‚¹å‡»æŸ¥çœ‹", "ç‚¹å‡»é“¾æ¥",
    "æ‰«ç ", "äºŒç»´ç ", "é•¿æŒ‰è¯†åˆ«", "è¯†åˆ«äºŒç»´ç ",
    "æ­£å“", "ä¿è¯", "æ•ˆæœ", "ç¥å¥‡", "æœ‰æ•ˆ", "å½»åº•", "è§£å†³"
]

# å¹¿å‘Šæ£€æµ‹å‡½æ•°
def detect_ad(text: str) -> Dict:
    """
    æ£€æµ‹æ–‡æœ¬æ˜¯å¦ä¸ºå¹¿å‘Šå†…å®¹
    è¿”å›åŒ…å«æ˜¯å¦ä¸ºå¹¿å‘Šã€ç½®ä¿¡åº¦ç­‰ä¿¡æ¯çš„å­—å…¸
    """
    try:
        # è½¬æ¢ä¸ºå°å†™è¿›è¡Œæ£€æµ‹
        text_lower = text.lower()
        
        # å…³é”®è¯åŒ¹é…
        matched_keywords = []
        for keyword in ad_keywords:
            if keyword in text_lower:
                matched_keywords.append(keyword)
        
        # è®¡ç®—ç½®ä¿¡åº¦ï¼ˆç®€å•å®ç°ï¼šæ ¹æ®åŒ¹é…åˆ°çš„å…³é”®è¯æ•°é‡å’Œæ–‡æœ¬é•¿åº¦è®¡ç®—ï¼‰
        confidence = 0.0
        if text_lower.strip():
            # åŸºç¡€ç½®ä¿¡åº¦ï¼šåŒ¹é…å…³é”®è¯æ•° / æ€»å…³é”®è¯æ•°
            base_confidence = len(matched_keywords) / len(ad_keywords)
            # åŠ æƒï¼šæ ¹æ®å…³é”®è¯åœ¨æ–‡æœ¬ä¸­çš„å¯†åº¦è°ƒæ•´
            keyword_density = len(matched_keywords) / max(1, len(text_lower) / 10)  # æ¯10ä¸ªå­—ç¬¦çš„å…³é”®è¯æ•°
            confidence = min(1.0, base_confidence * 0.6 + keyword_density * 0.4)
        
        # åˆ¤æ–­æ˜¯å¦ä¸ºå¹¿å‘Šï¼ˆç½®ä¿¡åº¦å¤§äº0.3æˆ–åŒ¹é…åˆ°2ä¸ªä»¥ä¸Šå…³é”®è¯ï¼‰
        is_ad = confidence > 0.3 or len(matched_keywords) >= 2
        
        # å®é™…é¡¹ç›®ä¸­å¯ä»¥ä½¿ç”¨æ›´å¤æ‚çš„ç®—æ³•æˆ–è°ƒç”¨ä¸“ä¸šçš„å¹¿å‘Šæ£€æµ‹API
        # ä¾‹å¦‚ï¼š
        # response = openai.chat.completions.create(
        #     model="gpt-3.5-turbo",
        #     messages=[
        #         {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªå¹¿å‘Šå†…å®¹æ£€æµ‹å™¨ï¼Œä¸“é—¨åˆ¤æ–­æ–‡æœ¬æ˜¯å¦ä¸ºå¹¿å‘Šã€‚"},
        #         {"role": "user", "content": f"è¯·åˆ¤æ–­ä»¥ä¸‹æ–‡æœ¬æ˜¯å¦ä¸ºå¹¿å‘Šï¼Œè¿”å›JSONæ ¼å¼ï¼š{{\"is_ad\": true/false, \"confidence\": 0.0-1.0, \"reason\": \"æ£€æµ‹ç†ç”±\"}}\n\næ–‡æœ¬ï¼š{text}"}
        #     ],
        #     max_tokens=150,
        #     temperature=0.3
        # )
        # result = json.loads(response.choices[0].message.content.strip())
        
        return {
            "is_ad": is_ad,
            "confidence": confidence,
            "matched_keywords": matched_keywords,
            "reason": f"åŒ¹é…åˆ°{len(matched_keywords)}ä¸ªå¹¿å‘Šå…³é”®è¯: {', '.join(matched_keywords)}"
        }
    except Exception as e:
        # å‡ºé”™æ—¶é»˜è®¤è¿”å›éå¹¿å‘Š
        return {
            "is_ad": False,
            "confidence": 0.0,
            "matched_keywords": [],
            "reason": f"å¹¿å‘Šæ£€æµ‹å‡ºé”™: {str(e)}"
        }

# æƒ…æ„Ÿåˆ†æå‡½æ•°
def analyze_emotion(text: str):
    """
    åˆ†ææ–‡æœ¬çš„æƒ…æ„ŸçŠ¶æ€ï¼Œè¿”å›æƒ…ç»ªç±»å‹å’Œè´Ÿé¢ç¨‹åº¦è¯„åˆ†
    
    è¯„åˆ†è§„åˆ™ï¼š
    - åˆ†æ•°èŒƒå›´ä¸º1-10
    - åˆ†æ•°è¶Šé«˜è¡¨ç¤ºæƒ…ç»ªè¶Šè´Ÿé¢
    - 1-3åˆ†ï¼šç§¯ææ­£é¢çš„æƒ…ç»ª
    - 4-5åˆ†ï¼šä¸­æ€§æˆ–è½»å¾®æƒ…ç»ªæ³¢åŠ¨
    - 6-8åˆ†ï¼šæ˜æ˜¾çš„è´Ÿé¢æƒ…ç»ª
    - 9-10åˆ†ï¼šå¼ºçƒˆçš„è´Ÿé¢æƒ…ç»ª
    """
    try:
        # æ„å»ºæƒ…æ„Ÿåˆ†ææç¤ºè¯
        prompt = f"""è¯·åˆ†æä»¥ä¸‹æ–‡æœ¬çš„æƒ…æ„ŸçŠ¶æ€ï¼š
{text}

è¯·æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¿”å›åˆ†æç»“æœï¼š
1. æƒ…ç»ªç±»å‹ï¼š[ç§¯æ/ä¸­æ€§/æ¶ˆæ/å…¶ä»–å…·ä½“æƒ…ç»ªç±»å‹]
2. è´Ÿé¢ç¨‹åº¦è¯„åˆ†ï¼š[1-10çš„æ•°å­—ï¼Œ1è¡¨ç¤ºæœ€ç§¯æï¼Œ10è¡¨ç¤ºæœ€è´Ÿé¢]
3. æƒ…ç»ªæè¿°ï¼š[å¯¹æƒ…ç»ªçš„ç®€è¦æè¿°]

è¯„åˆ†è§„åˆ™ï¼š
- åˆ†æ•°èŒƒå›´ä¸º1-10
- åˆ†æ•°è¶Šé«˜è¡¨ç¤ºæƒ…ç»ªè¶Šè´Ÿé¢
- 1-3åˆ†ï¼šç§¯ææ­£é¢çš„æƒ…ç»ª
- 4-5åˆ†ï¼šä¸­æ€§æˆ–è½»å¾®æƒ…ç»ªæ³¢åŠ¨
- 6-8åˆ†ï¼šæ˜æ˜¾çš„è´Ÿé¢æƒ…ç»ª
- 9-10åˆ†ï¼šå¼ºçƒˆçš„è´Ÿé¢æƒ…ç»ª"""
        
        # æ¨¡æ‹Ÿæƒ…æ„Ÿåˆ†æç»“æœï¼ˆå®é™…ä½¿ç”¨æ—¶åº”æ›¿æ¢ä¸ºçœŸå®APIè°ƒç”¨ï¼‰
        # è¿™é‡Œæ ¹æ®æ–‡æœ¬é•¿åº¦å’Œå†…å®¹ç‰¹å¾ç”Ÿæˆæ¨¡æ‹Ÿç»“æœ
        
        # ç®€å•çš„æ¨¡æ‹Ÿé€»è¾‘
        if "éš¾è¿‡" in text or "ä¼¤å¿ƒ" in text or "ä¸å¼€å¿ƒ" in text:
            emotion_type = "æ¶ˆæ"
            negative_score = 7
            emotion_description = "è¡¨è¾¾äº†æ‚²ä¼¤æˆ–ä¸å¼€å¿ƒçš„æƒ…ç»ª"
        elif "å¼€å¿ƒ" in text or "é«˜å…´" in text or "å¿«ä¹" in text:
            emotion_type = "ç§¯æ"
            negative_score = 2
            emotion_description = "è¡¨è¾¾äº†æ„‰æ‚¦æˆ–å¼€å¿ƒçš„æƒ…ç»ª"
        elif "ç”Ÿæ°”" in text or "æ„¤æ€’" in text or "çƒ¦" in text:
            emotion_type = "æ¶ˆæ"
            negative_score = 8
            emotion_description = "è¡¨è¾¾äº†æ„¤æ€’æˆ–çƒ¦èºçš„æƒ…ç»ª"
        elif "è°¢è°¢" in text or "æ„Ÿè°¢" in text:
            emotion_type = "ç§¯æ"
            negative_score = 1
            emotion_description = "è¡¨è¾¾äº†æ„Ÿæ¿€çš„æƒ…ç»ª"
        elif "å‹åŠ›" in text or "ç´¯" in text or "ç–²æƒ«" in text:
            emotion_type = "æ¶ˆæ"
            negative_score = 6
            emotion_description = "è¡¨è¾¾äº†å‹åŠ›æˆ–ç–²æƒ«çš„æƒ…ç»ª"
        else:
            # é»˜è®¤ä¸­æ€§
            emotion_type = "ä¸­æ€§"
            negative_score = 4
            emotion_description = "æƒ…ç»ªè¾ƒä¸ºå¹³é™ï¼Œæ²¡æœ‰æ˜æ˜¾çš„ç§¯ææˆ–æ¶ˆæå€¾å‘"
        
        # å®é™…è°ƒç”¨ç¤ºä¾‹ï¼š
        # response = openai.chat.completions.create(
        #     model="gpt-3.5-turbo",
        #     messages=[{"role": "user", "content": prompt}],
        #     max_tokens=150,
        #     temperature=0.3
        # )
        # response_text = response.choices[0].message.content.strip()
        # 
        # # è§£æç»“æœï¼ˆè¿™é‡Œéœ€è¦æ ¹æ®å®é™…è¿”å›æ ¼å¼è¿›è¡Œè°ƒæ•´ï¼‰
        # # ç¤ºä¾‹è§£æé€»è¾‘
        # emotion_type = ""
        # negative_score = 5
        # emotion_description = ""
        # 
        # for line in response_text.split('\n'):
        #     if line.startswith("1. æƒ…ç»ªç±»å‹ï¼š"):
        #         emotion_type = line.replace("1. æƒ…ç»ªç±»å‹ï¼š", "").strip()
        #     elif line.startswith("2. è´Ÿé¢ç¨‹åº¦è¯„åˆ†ï¼š"):
        #         try:
        #             negative_score = int(line.replace("2. è´Ÿé¢ç¨‹åº¦è¯„åˆ†ï¼š", "").strip())
        #         except:
        #             negative_score = 5
        #     elif line.startswith("3. æƒ…ç»ªæè¿°ï¼š"):
        #         emotion_description = line.replace("3. æƒ…ç»ªæè¿°ï¼š", "").strip()
        
        return EmotionAnalysisResult(
            emotion_type=emotion_type,
            negative_score=negative_score,
            emotion_description=emotion_description
        )
    except Exception as e:
        # å‡ºé”™æ—¶è¿”å›é»˜è®¤ä¸­æ€§ç»“æœ
        return EmotionAnalysisResult(
            emotion_type="ä¸­æ€§",
            negative_score=4,
            emotion_description=f"æƒ…æ„Ÿåˆ†æå‡ºé”™: {str(e)}"
        )

# ç”Ÿæˆå›å¤å†…å®¹çš„æ ¸å¿ƒå‡½æ•°
def generate_reply(circle_content: str, reply_style: str, 
                  is_first_reply: bool, previous_replies: List[str] = None,
                  emotion_analysis: EmotionAnalysisResult = None):
    """
    è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆå›å¤å†…å®¹
    """
    try:
        # æ„å»ºæç¤ºè¯ï¼ŒåŠ å…¥æƒ…æ„Ÿåˆ†æä¿¡æ¯
        emotion_info = ""
        if emotion_analysis:
            emotion_info = "\næœ‹å‹åœˆæƒ…æ„Ÿåˆ†æç»“æœ:\n- æƒ…ç»ªç±»å‹: " + emotion_analysis.emotion_type + "\n- è´Ÿé¢ç¨‹åº¦è¯„åˆ†: " + str(emotion_analysis.negative_score) + "\n- æƒ…ç»ªæè¿°: " + emotion_analysis.emotion_description + "\n"
            
        if is_first_reply:
            prompt = "è¯·ç”Ÿæˆä¸€ä¸ª" + reply_style + "é£æ ¼çš„æœ‹å‹åœˆå›å¤ã€‚\n"
            prompt += "æœ‹å‹åœˆå†…å®¹: " + circle_content + "\n"
            prompt += emotion_info + "\n"
            prompt += "å›å¤è¦æ±‚:\n"
            prompt += "1. å›å¤é£æ ¼å¿…é¡»æ˜¯" + reply_style + "\n"
            prompt += "2. å›å¤åº”è€ƒè™‘æœ‹å‹åœˆå†…å®¹çš„æƒ…æ„ŸçŠ¶æ€ï¼Œå¯¹äºè´Ÿé¢æƒ…ç»ªåº”ç»™äºˆé€‚å½“å®‰æ…°æˆ–æ”¯æŒ\n"
            prompt += "3. å†…å®¹é™å®š50å­—ä»¥å†…\n"
            prompt += "4. å¯ä»¥åŒ…å«æ–‡å­—å’Œå¾®ä¿¡è¡¨æƒ…åŒ…\n"
            prompt += "5. å†…å®¹å¿…é¡»ç¬¦åˆä¸­å›½æ³•å¾‹æ³•è§„\n"
            prompt += "6. ç›´æ¥è¾“å‡ºå›å¤å†…å®¹ï¼Œä¸è¦æ·»åŠ å…¶ä»–è§£é‡Š"
        else:
            # æ„å»ºåŒ…å«å†å²å›å¤çš„æç¤ºè¯
            history_text = "\nä¹‹å‰çš„å›å¤:\n"
            for i, reply in enumerate(previous_replies[-10:], 1):  # åªä¿ç•™æœ€è¿‘10æ¡
                history_text += "%d. %s\n" % (i, reply)
            
            prompt = "è¯·ç”Ÿæˆä¸€ä¸ª" + reply_style + "é£æ ¼çš„æœ‹å‹åœˆå›å¤ã€‚\n"
            prompt += "æœ‹å‹åœˆå†…å®¹: " + circle_content + "\n"
            prompt += emotion_info + "\n"
            prompt += history_text + "\n"
            prompt += "å›å¤è¦æ±‚:\n"
            prompt += "1. å›å¤é£æ ¼å¿…é¡»æ˜¯" + reply_style + "\n"
            prompt += "2. å›å¤åº”è€ƒè™‘æœ‹å‹åœˆå†…å®¹çš„æƒ…æ„ŸçŠ¶æ€ï¼Œå¯¹äºè´Ÿé¢æƒ…ç»ªåº”ç»™äºˆé€‚å½“å®‰æ…°æˆ–æ”¯æŒ\n"
            prompt += "3. å†…å®¹é™å®š50å­—ä»¥å†…\n"
            prompt += "4. å¯ä»¥åŒ…å«æ–‡å­—å’Œå¾®ä¿¡è¡¨æƒ…åŒ…\n"
            prompt += "5. å†…å®¹å¿…é¡»ç¬¦åˆä¸­å›½æ³•å¾‹æ³•è§„\n"
            prompt += "6. ç›´æ¥è¾“å‡ºå›å¤å†…å®¹ï¼Œä¸è¦æ·»åŠ å…¶ä»–è§£é‡Š"
        
        # è°ƒç”¨OpenAI API (æ¨¡æ‹Ÿå®ç°ï¼Œå®é™…ä½¿ç”¨æ—¶éœ€è¦æ ¹æ®å…·ä½“APIè°ƒæ•´)
        # è¿™é‡Œä¸ºäº†æ¼”ç¤ºï¼Œè¿”å›æ¨¡æ‹Ÿå†…å®¹
        # å®é™…é¡¹ç›®ä¸­æ›¿æ¢ä¸ºçœŸå®çš„APIè°ƒç”¨
        
        # æ¨¡æ‹Ÿå›å¤ç”Ÿæˆï¼Œæ ¹æ®æƒ…æ„Ÿåˆ†æç»“æœè°ƒæ•´å›å¤å†…å®¹
        if emotion_analysis:
            # æ ¹æ®æƒ…æ„ŸçŠ¶æ€ç”Ÿæˆä¸åŒçš„æ¨¡æ‹Ÿå›å¤
            if emotion_analysis.negative_score >= 6:  # è´Ÿé¢æƒ…ç»ª
                mock_replies = {
                    "å¹½é»˜": "å¸Œæœ›æˆ‘çš„å›å¤èƒ½è®©ä½ å¿ƒæƒ…å¥½ä¸€ç‚¹~ ğŸ˜‰",
                    "ä¸¥è‚ƒ": "ç†è§£ä½ çš„æ„Ÿå—ï¼Œä¸€åˆ‡éƒ½ä¼šå¥½èµ·æ¥çš„ã€‚",
                    "æš§æ˜§": "å¾ˆå¿ƒç–¼ä½ çš„çŠ¶æ€ï¼Œéœ€è¦ä¸€ä¸ªæ‹¥æŠ±å—ï¼Ÿ ğŸ«‚",
                    "æ¸©é¦¨": "åˆ«éš¾è¿‡ï¼Œæˆ‘åœ¨è¿™é‡Œé™ªä¼´ä½ ~ ğŸŒŸ",
                    "æ‰¹è¯„": "è™½ç„¶æƒ…ç»ªä¸å¥½ï¼Œä½†æˆ‘ä»¬å¯ä»¥ä¸€èµ·åˆ†æé—®é¢˜ã€‚"
                }
            elif emotion_analysis.negative_score <= 3:  # ç§¯ææƒ…ç»ª
                mock_replies = {
                    "å¹½é»˜": "çœ‹åˆ°ä½ å¼€å¿ƒæˆ‘ä¹Ÿå¾ˆå¼€å¿ƒï¼ğŸ˜‚",
                    "ä¸¥è‚ƒ": "ä½ çš„ç§¯ææ€åº¦å¾ˆå€¼å¾—èµèµã€‚",
                    "æš§æ˜§": "ä½ çš„å¿«ä¹æ„ŸæŸ“äº†æˆ‘~ ğŸ˜Š",
                    "æ¸©é¦¨": "çœŸå¥½ï¼Œèƒ½åˆ†äº«ä½ çš„å¿«ä¹~ ğŸŒŸ",
                    "æ‰¹è¯„": "è™½ç„¶æ•´ä½“ç§¯æï¼Œä½†è¿˜æœ‰äº›å°å»ºè®®æƒ³å’Œä½ æ¢è®¨ã€‚"
                }
            else:  # ä¸­æ€§æƒ…ç»ª
                mock_replies = {
                    "å¹½é»˜": "å“ˆå“ˆï¼Œè¯´å¾—å¤ªå¯¹äº†ï¼ğŸ˜‚",
                    "ä¸¥è‚ƒ": "ä½ è¯´å¾—å¾ˆæœ‰é“ç†ï¼Œå€¼å¾—æ·±æ€ã€‚",
                    "æš§æ˜§": "è¿™ä¸ªåˆ†äº«å¾ˆç‰¹åˆ«å‘¢~ ğŸ˜Š",
                    "æ¸©é¦¨": "çœ‹å®Œå¾ˆæ¸©æš–ï¼Œè°¢è°¢åˆ†äº«~ ğŸŒŸ",
                    "æ‰¹è¯„": "æˆ‘è®¤ä¸ºè¿™ä¸ªè§‚ç‚¹è¿˜æœ‰å¾…å•†æ¦·ã€‚"
                }
        else:
            # é»˜è®¤å›å¤
            mock_replies = {
                "å¹½é»˜": "å“ˆå“ˆï¼Œè¯´å¾—å¤ªå¯¹äº†ï¼ğŸ˜‚",
                "ä¸¥è‚ƒ": "ä½ è¯´å¾—å¾ˆæœ‰é“ç†ï¼Œå€¼å¾—æ·±æ€ã€‚",
                "æš§æ˜§": "è¿™ä¸ªåˆ†äº«å¾ˆç‰¹åˆ«å‘¢~ ğŸ˜Š",
                "æ¸©é¦¨": "çœ‹å®Œå¾ˆæ¸©æš–ï¼Œè°¢è°¢åˆ†äº«~ ğŸŒŸ",
                "æ‰¹è¯„": "æˆ‘è®¤ä¸ºè¿™ä¸ªè§‚ç‚¹è¿˜æœ‰å¾…å•†æ¦·ã€‚"
            }
        
        reply = mock_replies.get(reply_style, "å¾ˆæœ‰æ„æ€çš„åˆ†äº«ï¼")
        
        # å®é™…è°ƒç”¨ç¤ºä¾‹ï¼ˆéœ€è¦APIå¯†é’¥ï¼‰:
        # response = openai.chat.completions.create(
        #     model="gpt-3.5-turbo",
        #     messages=[{"role": "user", "content": prompt}],
        #     max_tokens=100,
        #     temperature=0.7
        # )
        # reply = response.choices[0].message.content.strip()
        
        # ç¡®ä¿å›å¤é•¿åº¦ä¸è¶…è¿‡50å­—
        if len(reply) > 50:
            reply = reply[:50]
        
        return reply
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ç”Ÿæˆå›å¤æ—¶å‡ºé”™: {str(e)}")

# ç”Ÿæˆå›å¤æ‘˜è¦çš„å‡½æ•°
def generate_reply_summary(replies: List[str]):
    """
    å½“å›å¤æ•°é‡è¶…è¿‡20æ¡æ—¶ï¼Œç”Ÿæˆæ‘˜è¦
    """
    try:
        prompt = f"""è¯·å°†ä»¥ä¸‹æœ‹å‹åœˆå›å¤å†…å®¹ç”Ÿæˆä¸€ä¸ªç®€çŸ­çš„æ‘˜è¦ï¼Œä¿ç•™å…³é”®ä¿¡æ¯å’Œè®¨è®ºä¸»é¢˜ã€‚
å›å¤å†…å®¹:
{"\n".join(replies)}
æ‘˜è¦è¦æ±‚ç®€æ´ï¼Œä¸è¶…è¿‡100å­—ã€‚"""
        
        # æ¨¡æ‹Ÿæ‘˜è¦ç”Ÿæˆ
        summary = f"å…³äºè¿™æ¡æœ‹å‹åœˆçš„è®¨è®ºæ¶‰åŠäº†å¤šä¸ªæ–¹é¢ï¼Œå…±æœ‰{len(replies)}æ¡å›å¤..."
        
        # å®é™…è°ƒç”¨ç¤ºä¾‹:
        # response = openai.chat.completions.create(
        #     model="gpt-3.5-turbo",
        #     messages=[{"role": "user", "content": prompt}],
        #     max_tokens=150,
        #     temperature=0.3
        # )
        # summary = response.choices[0].message.content.strip()
        
        return summary
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ç”Ÿæˆæ‘˜è¦æ—¶å‡ºé”™: {str(e)}")

# ä¿å­˜å›å¤å†å²
def save_reply_history(user_id: str, post_id: str, reply: str):
    """
    ä¿å­˜å›å¤å†å²åˆ°å†…å­˜æˆ–Redis
    """
    key = f"{user_id}:{post_id}"
    
    # è·å–ç°æœ‰å†å²
    if REDIS_AVAILABLE:
        # ä½¿ç”¨Rediså­˜å‚¨
        history_data = redis_client.get(key)
        if history_data:
            history = json.loads(history_data)
        else:
            history = []
        
        # æ·»åŠ æ–°å›å¤
        history.append({
            "reply": reply,
            "timestamp": datetime.now().isoformat()
        })
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦ç”Ÿæˆæ‘˜è¦
        if len(history) > 20:
            # æå–æ‰€æœ‰å›å¤å†…å®¹
            replies = [item["reply"] for item in history]
            # ç”Ÿæˆæ‘˜è¦
            summary = generate_reply_summary(replies)
            # ä¿ç•™æ‘˜è¦å’Œæœ€è¿‘å‡ æ¡å›å¤
            history = [
                {"type": "summary", "content": summary, "timestamp": datetime.now().isoformat()},
                *history[-5:]
            ]
        
        # ä¿å­˜å›Redis
        redis_client.setex(key, 60*60*24*7, json.dumps(history))  # ä¿å­˜7å¤©
    else:
        # ä½¿ç”¨å†…å­˜å­˜å‚¨
        if key not in memory_storage:
            memory_storage[key] = []
        
        memory_storage[key].append({
            "reply": reply,
            "timestamp": datetime.now().isoformat()
        })
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦ç”Ÿæˆæ‘˜è¦
        if len(memory_storage[key]) > 20:
            # æå–æ‰€æœ‰å›å¤å†…å®¹
            replies = [item["reply"] for item in memory_storage[key]]
            # ç”Ÿæˆæ‘˜è¦
            summary = generate_reply_summary(replies)
            # ä¿ç•™æ‘˜è¦å’Œæœ€è¿‘å‡ æ¡å›å¤
            memory_storage[key] = [
                {"type": "summary", "content": summary, "timestamp": datetime.now().isoformat()},
                *memory_storage[key][-5:]
            ]

# è·å–å›å¤å†å²
def get_reply_history(user_id: str, post_id: str) -> List[str]:
    """
    ä»å†…å­˜æˆ–Redisè·å–å›å¤å†å²
    """
    key = f"{user_id}:{post_id}"
    history = []
    
    if REDIS_AVAILABLE:
        # ä»Redisè·å–
        history_data = redis_client.get(key)
        if history_data:
            history = json.loads(history_data)
    else:
        # ä»å†…å­˜è·å–
        if key in memory_storage:
            history = memory_storage[key]
    
    # æå–å›å¤å†…å®¹ï¼Œè¿‡æ»¤æ‘˜è¦
    replies = []
    for item in history:
        if item.get("type") == "summary":
            # å¦‚æœæ˜¯æ‘˜è¦ï¼Œå°†å…¶ä½œä¸ºä¸Šä¸‹æ–‡çš„ä¸€éƒ¨åˆ†
            replies.append(f"[è®¨è®ºæ‘˜è¦] {item['content']}")
        else:
            replies.append(item.get("reply", ""))
    
    return replies

# æ£€æŸ¥æ˜¯å¦æ˜¯ç¬¬ä¸€æ¬¡å›å¤
def is_first_reply(user_id: str, post_id: str) -> bool:
    """
    æ£€æŸ¥ç”¨æˆ·å¯¹æŸæ¡æœ‹å‹åœˆæ˜¯å¦æ˜¯ç¬¬ä¸€æ¬¡å›å¤
    """
    history = get_reply_history(user_id, post_id)
    # å¦‚æœæ²¡æœ‰å†å²å›å¤ï¼Œæˆ–è€…åªæœ‰æ‘˜è¦ï¼ˆæ‘˜è¦ä¸ç®—å®é™…å›å¤ï¼‰ï¼Œåˆ™è§†ä¸ºç¬¬ä¸€æ¬¡å›å¤
    actual_replies = [r for r in history if not r.startswith("[è®¨è®ºæ‘˜è¦]")]
    return len(actual_replies) == 0

# APIç«¯ç‚¹ï¼šç”Ÿæˆå›å¤
@app.post("/generate_reply", response_model=ReplyResponse)
async def generate_reply_endpoint(request: ReplyRequest):
    # éªŒè¯å›å¤é£æ ¼
    if request.reply_style not in valid_styles:
        raise HTTPException(status_code=400, detail=f"æ— æ•ˆçš„å›å¤é£æ ¼ï¼Œå¿…é¡»æ˜¯ä»¥ä¸‹ä¹‹ä¸€: {', '.join(valid_styles)}")
    
    # éªŒè¯æœ‹å‹åœˆå†…å®¹
    if not request.circle_content or len(request.circle_content) == 0:
        raise HTTPException(status_code=400, detail="æœ‹å‹åœˆå†…å®¹ä¸èƒ½ä¸ºç©º")
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯ç¬¬ä¸€æ¬¡å›å¤
    first_reply = is_first_reply(request.user_id, request.post_id)
    
    # è·å–å†å²å›å¤
    history_replies = get_reply_history(request.user_id, request.post_id)
    
    # è¿›è¡Œæƒ…æ„Ÿåˆ†æ
    emotion_analysis_result = analyze_emotion(request.circle_content)
    
    # ç”Ÿæˆå›å¤ï¼Œä¼ å…¥æƒ…æ„Ÿåˆ†æç»“æœ
    reply_content = generate_reply(
        circle_content=request.circle_content,
        reply_style=request.reply_style,
        is_first_reply=first_reply,
        previous_replies=history_replies,
        emotion_analysis=emotion_analysis_result
    )
    
    # ä¿å­˜æ–°å›å¤
    save_reply_history(request.user_id, request.post_id, reply_content)
    
    # è¿”å›ç»“æœï¼ŒåŒ…å«æƒ…æ„Ÿåˆ†æä¿¡æ¯
    return ReplyResponse(
        reply_content=reply_content,
        is_first_reply=first_reply,
        timestamp=datetime.now(),
        emotion_analysis=emotion_analysis_result
    )

# APIç«¯ç‚¹ï¼šè·å–å›å¤å†å²
@app.get("/reply_history/{user_id}/{post_id}")
async def get_reply_history_endpoint(user_id: str, post_id: str):
    history = get_reply_history(user_id, post_id)
    return {
        "user_id": user_id,
        "post_id": post_id,
        "reply_count": len(history),
        "replies": history
    }

# APIç«¯ç‚¹ï¼šåˆ é™¤å›å¤å†å²
@app.delete("/reply_history/{user_id}/{post_id}")
async def delete_reply_history_endpoint(user_id: str, post_id: str):
    key = f"{user_id}:{post_id}"
    
    if REDIS_AVAILABLE:
        redis_client.delete(key)
    elif key in memory_storage:
        del memory_storage[key]
    
    return {"message": "å›å¤å†å²å·²åˆ é™¤"}

# å¥åº·æ£€æŸ¥ç«¯ç‚¹
@app.get("/health")
async def health_check():
    redis_status = "connected" if REDIS_AVAILABLE else "disconnected"
    return {
        "status": "healthy",
        "redis_status": redis_status,
        "timestamp": datetime.now().isoformat()
    }

# APIç«¯ç‚¹ï¼šæ£€æµ‹å¹¿å‘Š
@app.post("/detect_ad", response_model=AdDetectionResponse)
async def detect_ad_endpoint(request: AdDetectionRequest):
    # éªŒè¯æœ‹å‹åœˆå†…å®¹
    if not request.circle_content or len(request.circle_content.strip()) == 0:
        raise HTTPException(status_code=400, detail="æœ‹å‹åœˆå†…å®¹ä¸èƒ½ä¸ºç©º")
    
    # æ‰§è¡Œå¹¿å‘Šæ£€æµ‹
    detection_result = detect_ad(request.circle_content)
    
    # æ„å»ºå“åº”
    response_text = "æˆ‘ä¸æ„Ÿå…´è¶£" if detection_result["is_ad"] else "éå¹¿å‘Šå†…å®¹"
    
    return AdDetectionResponse(
        is_ad=detection_result["is_ad"],
        response_text=response_text,
        confidence=detection_result["confidence"],
        timestamp=datetime.now()
    )

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="127.0.0.1", port=8000)