from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
from typing import Optional, List, Dict
import redis
import json
import os
from datetime import datetime
from dotenv import load_dotenv
import openai

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# åˆå§‹åŒ–FastAPIåº”ç”¨
app = FastAPI(
    title="å¾®ä¿¡æœ‹å‹åœˆè‡ªåŠ¨å›å¤æœåŠ¡",
    description="æä¾›æœ‹å‹åœˆè‡ªåŠ¨å›å¤ç”ŸæˆåŠŸèƒ½",
    version="1.0.0"
)

# åˆå§‹åŒ–Redisè¿æ¥ï¼ˆå¦‚æœç¯å¢ƒä¸­æœ‰é…ç½®ï¼‰
REDIS_URL = os.getenv("REDIS_URL", "redis://localhost:6379/0")
REDIS_AVAILABLE = False
redis_client = None

try:
    redis_client = redis.from_url(REDIS_URL, decode_responses=True, socket_connect_timeout=5)
    redis_client.ping()
    REDIS_AVAILABLE = True
    print("Redisè¿æ¥æˆåŠŸ")
except Exception as e:
    print(f"Redisè¿æ¥å¤±è´¥ï¼Œå°†ä½¿ç”¨å†…å­˜å­˜å‚¨: {str(e)}")

# å†…å­˜å­˜å‚¨ï¼Œç”¨äºç¼“å­˜ç”¨æˆ·çš„å›å¤å†å²
memory_storage: Dict[str, List[Dict]] = {}

# é…ç½®OpenAI APIï¼ˆæ ¹æ®å®é™…ä½¿ç”¨çš„å¤§æ¨¡å‹è°ƒæ•´ï¼‰
openai.api_key = os.getenv("OPENAI_API_KEY", "")

# å®šä¹‰æ•°æ®æ¨¡å‹
class ReplyRequest(BaseModel):
    circle_content: str = Field(..., description="æœ‹å‹åœˆå†…å®¹")
    reply_style: str = Field(..., description="å›å¤é£æ ¼ï¼šå¹½é»˜ï¼Œä¸¥è‚ƒï¼Œæš§æ˜§ï¼Œæ¸©é¦¨ï¼Œæ‰¹è¯„")
    user_id: str = Field(..., description="ç”¨æˆ·å”¯ä¸€æ ‡è¯†")
    post_id: str = Field(..., description="æœ‹å‹åœˆå¸–å­å”¯ä¸€æ ‡è¯†")
    previous_replies: Optional[List[str]] = Field(default=[], description="ä¹‹å‰çš„å›å¤å†…å®¹åˆ—è¡¨")

class ReplyResponse(BaseModel):
    reply_content: str = Field(..., description="ç”Ÿæˆçš„å›å¤å†…å®¹")
    is_first_reply: bool = Field(..., description="æ˜¯å¦æ˜¯ç¬¬ä¸€æ¬¡å›å¤")
    timestamp: datetime = Field(..., description="ç”Ÿæˆæ—¶é—´æˆ³")

# éªŒè¯å›å¤é£æ ¼
valid_styles = ["å¹½é»˜", "ä¸¥è‚ƒ", "æš§æ˜§", "æ¸©é¦¨", "æ‰¹è¯„"]

# ç”Ÿæˆå›å¤å†…å®¹çš„æ ¸å¿ƒå‡½æ•°
def generate_reply(circle_content: str, reply_style: str, 
                  is_first_reply: bool, previous_replies: List[str] = None):
    """
    è°ƒç”¨å¤§æ¨¡å‹ç”Ÿæˆå›å¤å†…å®¹
    """
    try:
        # æ„å»ºæç¤ºè¯
        if is_first_reply:
            prompt = f"""è¯·ç”Ÿæˆä¸€ä¸ª{reply_style}é£æ ¼çš„æœ‹å‹åœˆå›å¤ã€‚
æœ‹å‹åœˆå†…å®¹: {circle_content}
å›å¤è¦æ±‚:
1. å›å¤é£æ ¼å¿…é¡»æ˜¯{reply_style}
2. å†…å®¹é™å®š50å­—ä»¥å†…
3. å¯ä»¥åŒ…å«æ–‡å­—å’Œå¾®ä¿¡è¡¨æƒ…åŒ…
4. å†…å®¹å¿…é¡»ç¬¦åˆä¸­å›½æ³•å¾‹æ³•è§„
5. ç›´æ¥è¾“å‡ºå›å¤å†…å®¹ï¼Œä¸è¦æ·»åŠ å…¶ä»–è§£é‡Š"""
        else:
            # æ„å»ºåŒ…å«å†å²å›å¤çš„æç¤ºè¯
            history_text = "\nä¹‹å‰çš„å›å¤:\n"
            for i, reply in enumerate(previous_replies[-10:], 1):  # åªä¿ç•™æœ€è¿‘10æ¡
                history_text += f"{i}. {reply}\n"
            
            prompt = f"""è¯·ç”Ÿæˆä¸€ä¸ª{reply_style}é£æ ¼çš„æœ‹å‹åœˆå›å¤ã€‚
æœ‹å‹åœˆå†…å®¹: {circle_content}
{history_text}
å›å¤è¦æ±‚:
1. å›å¤é£æ ¼å¿…é¡»æ˜¯{reply_style}
2. å†…å®¹é™å®š50å­—ä»¥å†…
3. å¯ä»¥åŒ…å«æ–‡å­—å’Œå¾®ä¿¡è¡¨æƒ…åŒ…
4. å†…å®¹å¿…é¡»ç¬¦åˆä¸­å›½æ³•å¾‹æ³•è§„
5. ç›´æ¥è¾“å‡ºå›å¤å†…å®¹ï¼Œä¸è¦æ·»åŠ å…¶ä»–è§£é‡Š"""
        
        # è°ƒç”¨OpenAI API (æ¨¡æ‹Ÿå®ç°ï¼Œå®é™…ä½¿ç”¨æ—¶éœ€è¦æ ¹æ®å…·ä½“APIè°ƒæ•´)
        # è¿™é‡Œä¸ºäº†æ¼”ç¤ºï¼Œè¿”å›æ¨¡æ‹Ÿå†…å®¹
        # å®é™…é¡¹ç›®ä¸­æ›¿æ¢ä¸ºçœŸå®çš„APIè°ƒç”¨
        
        # æ¨¡æ‹Ÿå›å¤ç”Ÿæˆ
        mock_replies = {
            "å¹½é»˜": "å“ˆå“ˆï¼Œè¯´å¾—å¤ªå¯¹äº†ï¼ğŸ˜‚",
            "ä¸¥è‚ƒ": "ä½ è¯´å¾—å¾ˆæœ‰é“ç†ï¼Œå€¼å¾—æ·±æ€ã€‚",
            "æš§æ˜§": "è¿™ä¸ªåˆ†äº«å¾ˆç‰¹åˆ«å‘¢~ ğŸ˜Š",
            "æ¸©é¦¨": "çœ‹å®Œå¾ˆæ¸©æš–ï¼Œè°¢è°¢åˆ†äº«~ ğŸŒŸ",
            "æ‰¹è¯„": "æˆ‘è®¤ä¸ºè¿™ä¸ªè§‚ç‚¹è¿˜æœ‰å¾…å•†æ¦·ã€‚"
        }
        
        reply = mock_replies.get(reply_style, "å¾ˆæœ‰æ„æ€çš„åˆ†äº«ï¼")
        
        # å®é™…è°ƒç”¨ç¤ºä¾‹ï¼ˆéœ€è¦APIå¯†é’¥ï¼‰:
        # response = openai.chat.completions.create(
        #     model="gpt-3.5-turbo",
        #     messages=[{"role": "user", "content": prompt}],
        #     max_tokens=100,
        #     temperature=0.7
        # )
        # reply = response.choices[0].message.content.strip()
        
        # ç¡®ä¿å›å¤é•¿åº¦ä¸è¶…è¿‡50å­—
        if len(reply) > 50:
            reply = reply[:50]
        
        return reply
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ç”Ÿæˆå›å¤æ—¶å‡ºé”™: {str(e)}")

# ç”Ÿæˆå›å¤æ‘˜è¦çš„å‡½æ•°
def generate_reply_summary(replies: List[str]):
    """
    å½“å›å¤æ•°é‡è¶…è¿‡20æ¡æ—¶ï¼Œç”Ÿæˆæ‘˜è¦
    """
    try:
        prompt = f"""è¯·å°†ä»¥ä¸‹æœ‹å‹åœˆå›å¤å†…å®¹ç”Ÿæˆä¸€ä¸ªç®€çŸ­çš„æ‘˜è¦ï¼Œä¿ç•™å…³é”®ä¿¡æ¯å’Œè®¨è®ºä¸»é¢˜ã€‚
å›å¤å†…å®¹:
{"\n".join(replies)}
æ‘˜è¦è¦æ±‚ç®€æ´ï¼Œä¸è¶…è¿‡100å­—ã€‚"""
        
        # æ¨¡æ‹Ÿæ‘˜è¦ç”Ÿæˆ
        summary = f"å…³äºè¿™æ¡æœ‹å‹åœˆçš„è®¨è®ºæ¶‰åŠäº†å¤šä¸ªæ–¹é¢ï¼Œå…±æœ‰{len(replies)}æ¡å›å¤..."
        
        # å®é™…è°ƒç”¨ç¤ºä¾‹:
        # response = openai.chat.completions.create(
        #     model="gpt-3.5-turbo",
        #     messages=[{"role": "user", "content": prompt}],
        #     max_tokens=150,
        #     temperature=0.3
        # )
        # summary = response.choices[0].message.content.strip()
        
        return summary
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ç”Ÿæˆæ‘˜è¦æ—¶å‡ºé”™: {str(e)}")

# ä¿å­˜å›å¤å†å²
def save_reply_history(user_id: str, post_id: str, reply: str):
    """
    ä¿å­˜å›å¤å†å²åˆ°å†…å­˜æˆ–Redis
    """
    key = f"{user_id}:{post_id}"
    
    # è·å–ç°æœ‰å†å²
    if REDIS_AVAILABLE:
        # ä½¿ç”¨Rediså­˜å‚¨
        history_data = redis_client.get(key)
        if history_data:
            history = json.loads(history_data)
        else:
            history = []
        
        # æ·»åŠ æ–°å›å¤
        history.append({
            "reply": reply,
            "timestamp": datetime.now().isoformat()
        })
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦ç”Ÿæˆæ‘˜è¦
        if len(history) > 20:
            # æå–æ‰€æœ‰å›å¤å†…å®¹
            replies = [item["reply"] for item in history]
            # ç”Ÿæˆæ‘˜è¦
            summary = generate_reply_summary(replies)
            # ä¿ç•™æ‘˜è¦å’Œæœ€è¿‘å‡ æ¡å›å¤
            history = [
                {"type": "summary", "content": summary, "timestamp": datetime.now().isoformat()},
                *history[-5:]
            ]
        
        # ä¿å­˜å›Redis
        redis_client.setex(key, 60*60*24*7, json.dumps(history))  # ä¿å­˜7å¤©
    else:
        # ä½¿ç”¨å†…å­˜å­˜å‚¨
        if key not in memory_storage:
            memory_storage[key] = []
        
        memory_storage[key].append({
            "reply": reply,
            "timestamp": datetime.now().isoformat()
        })
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦ç”Ÿæˆæ‘˜è¦
        if len(memory_storage[key]) > 20:
            # æå–æ‰€æœ‰å›å¤å†…å®¹
            replies = [item["reply"] for item in memory_storage[key]]
            # ç”Ÿæˆæ‘˜è¦
            summary = generate_reply_summary(replies)
            # ä¿ç•™æ‘˜è¦å’Œæœ€è¿‘å‡ æ¡å›å¤
            memory_storage[key] = [
                {"type": "summary", "content": summary, "timestamp": datetime.now().isoformat()},
                *memory_storage[key][-5:]
            ]

# è·å–å›å¤å†å²
def get_reply_history(user_id: str, post_id: str) -> List[str]:
    """
    ä»å†…å­˜æˆ–Redisè·å–å›å¤å†å²
    """
    key = f"{user_id}:{post_id}"
    history = []
    
    if REDIS_AVAILABLE:
        # ä»Redisè·å–
        history_data = redis_client.get(key)
        if history_data:
            history = json.loads(history_data)
    else:
        # ä»å†…å­˜è·å–
        if key in memory_storage:
            history = memory_storage[key]
    
    # æå–å›å¤å†…å®¹ï¼Œè¿‡æ»¤æ‘˜è¦
    replies = []
    for item in history:
        if item.get("type") == "summary":
            # å¦‚æœæ˜¯æ‘˜è¦ï¼Œå°†å…¶ä½œä¸ºä¸Šä¸‹æ–‡çš„ä¸€éƒ¨åˆ†
            replies.append(f"[è®¨è®ºæ‘˜è¦] {item['content']}")
        else:
            replies.append(item.get("reply", ""))
    
    return replies

# æ£€æŸ¥æ˜¯å¦æ˜¯ç¬¬ä¸€æ¬¡å›å¤
def is_first_reply(user_id: str, post_id: str) -> bool:
    """
    æ£€æŸ¥ç”¨æˆ·å¯¹æŸæ¡æœ‹å‹åœˆæ˜¯å¦æ˜¯ç¬¬ä¸€æ¬¡å›å¤
    """
    history = get_reply_history(user_id, post_id)
    # å¦‚æœæ²¡æœ‰å†å²å›å¤ï¼Œæˆ–è€…åªæœ‰æ‘˜è¦ï¼ˆæ‘˜è¦ä¸ç®—å®é™…å›å¤ï¼‰ï¼Œåˆ™è§†ä¸ºç¬¬ä¸€æ¬¡å›å¤
    actual_replies = [r for r in history if not r.startswith("[è®¨è®ºæ‘˜è¦]")]
    return len(actual_replies) == 0

# APIç«¯ç‚¹ï¼šç”Ÿæˆå›å¤
@app.post("/generate_reply", response_model=ReplyResponse)
async def generate_reply_endpoint(request: ReplyRequest):
    # éªŒè¯å›å¤é£æ ¼
    if request.reply_style not in valid_styles:
        raise HTTPException(status_code=400, detail=f"æ— æ•ˆçš„å›å¤é£æ ¼ï¼Œå¿…é¡»æ˜¯ä»¥ä¸‹ä¹‹ä¸€: {', '.join(valid_styles)}")
    
    # éªŒè¯æœ‹å‹åœˆå†…å®¹
    if not request.circle_content or len(request.circle_content) == 0:
        raise HTTPException(status_code=400, detail="æœ‹å‹åœˆå†…å®¹ä¸èƒ½ä¸ºç©º")
    
    # æ£€æŸ¥æ˜¯å¦æ˜¯ç¬¬ä¸€æ¬¡å›å¤
    first_reply = is_first_reply(request.user_id, request.post_id)
    
    # è·å–å†å²å›å¤
    history_replies = get_reply_history(request.user_id, request.post_id)
    
    # ç”Ÿæˆå›å¤
    reply_content = generate_reply(
        circle_content=request.circle_content,
        reply_style=request.reply_style,
        is_first_reply=first_reply,
        previous_replies=history_replies
    )
    
    # ä¿å­˜æ–°å›å¤
    save_reply_history(request.user_id, request.post_id, reply_content)
    
    # è¿”å›ç»“æœ
    return ReplyResponse(
        reply_content=reply_content,
        is_first_reply=first_reply,
        timestamp=datetime.now()
    )

# APIç«¯ç‚¹ï¼šè·å–å›å¤å†å²
@app.get("/reply_history/{user_id}/{post_id}")
async def get_reply_history_endpoint(user_id: str, post_id: str):
    history = get_reply_history(user_id, post_id)
    return {
        "user_id": user_id,
        "post_id": post_id,
        "reply_count": len(history),
        "replies": history
    }

# APIç«¯ç‚¹ï¼šåˆ é™¤å›å¤å†å²
@app.delete("/reply_history/{user_id}/{post_id}")
async def delete_reply_history_endpoint(user_id: str, post_id: str):
    key = f"{user_id}:{post_id}"
    
    if REDIS_AVAILABLE:
        redis_client.delete(key)
    elif key in memory_storage:
        del memory_storage[key]
    
    return {"message": "å›å¤å†å²å·²åˆ é™¤"}

# å¥åº·æ£€æŸ¥ç«¯ç‚¹
@app.get("/health")
async def health_check():
    redis_status = "connected" if REDIS_AVAILABLE else "disconnected"
    return {
        "status": "healthy",
        "redis_status": redis_status,
        "timestamp": datetime.now().isoformat()
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="127.0.0.1", port=8000)